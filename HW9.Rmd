---
title: "HW 9_Onyango"
author: "Brenda Onyango"
date: "10/23/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 8.1

The three common tasks in a posterior analysis: estimation, hypothesis testing, and prediction. 

## Exercise 8.2

a) When only reporting the central tendency we don't get a sense of the variability of possible values of pi. Depending on sample size the posterior credible interval could be narrow or wider (figure 8.2 in the chapter demonstrates this). Having both central tendency and variability allows us to compare different posterior models of the same prior model. 

b) A 95% credible interval of (1, 3.4) means there's a 95% posterior probability that lambda is between 1 and 3.4 

## Exercise 8.3 

a) Yes this could be hypothesis tested. Ho is that 40% of dogs at the park do not have a license; Ha is that more than 40% of dogs at the park do not have a license. 

b) As its written this could not be hypothesis tested.

c) Yes. The null hypothesis is that 60% of people support a regulation; the alternative is that more than 60% of people support the legislation.

d) Yes. This would be a Gamma-Poisson model since we're given a rate. 

## Exercise 8.4

a) Posterior odds are the posterior probability divied by 1 minus the posterior probability and represent an updated understanding of pi,mu, or lambda that indicates how much more liklely or unlikely an hypothesis is.

b) Prior odds are the prior probability divided by 1 minus the prior probability and represent a prior understanding of pi, mu or lambda. 

c) Bayes Factor is a ratio of posterior odds to prior odds and comparing Bayes Factor to 1 helps make judgements about H0 and Ha. BF = 1 means that the plausibility of Ha didn't change in light of data; >1 means the plausibility of Ha is greater in light of data; and < 1 means the plausibility of Ha decreased in light of data. 

## Exercise 8.5

a) The two types of variability in posterior predictive models are sampling variability and posterior variability. Sampling variability is variability in the data; posterior variability is the variability in possible values of pi. 

b) A real life situation for posterior prediction are elections or weather forecasting. 

c) Posterior predictive models are conditional on both the data and the parameter since they take into account sampling variability (data) and the posterior variability which relates to the parameter. 

## Exercise 8.6

a) A 95% credible interval for π with π|y∼Beta(4,5)

First load packages needed for this exercise. 
```{r}
library(tidyverse)
library(bayesrules)
library(rstan)
library(bayesplot)
library(broom.mixed)
library(janitor)
library(HDInterval)

```

Next find the 95% credible interval. 

```{r}
# 0.025th & 0.975th quantiles of the Beta(4,5) posterior
qbeta(c(0.025, 0.975), 4, 5)
```

b)
```{r}
# 0.20th & 0.80th quantiles of the Beta(4,5) posterior
qbeta(c(0.20, 0.8), 4, 5)
```

c)
```{r}
# 0.025th & 0.975th quantiles of the Gamma(1,8) posterior
qgamma(c(0.025, 0.975), 1, 8)
```

## Exercise 8.7

a) 
```{r}
# 0.005th & 0.995th quantiles of the Gamma(1,5) posterior
qgamma(c(0.005, 0.995), 1, 5)

```

b) 
```{r}
# 0.025th & 0.975th quantiles of the Normal(10,2^2) posterior
qnorm(c(0.025, 0.975), 10, 2)
```

c)
```{r}
# 0.20th & 0.80th quantiles of the Normal(-3, 1^2) posterior
qnorm(c(0.20, 0.8), -3, 1)
```

#Corrected 

## Exercise 8.8

a)
```{r}
# finding density between 0 and .95.
qgamma(c(0,.95), 1, 5)
```

```{r}
#code from https://search.r-project.org/CRAN/refmans/HDInterval/html/hdi.html to do hdi
# for a vector:
set.seed(50)
tst <- rgamma(100, 1, 5) #randomly chose 100 for n
hdi(tst)
hdi(tst, credMass=0.95)
# For comparison, the symmetrical 95% CrI:
quantile(tst, c(0.025,0.975))
```

Next I'll plot  this to see density. 
```{r}
plot_gamma(1,5)
```

b)
```{r}
# 0.025th & 0.975th quantiles of the Gamma(1,5) posterior with the middle approach 
qgamma(c(0.025, 0.975), 1, 5)
```

c) The two intervals are not the same; the second interval (middle approach) has an upper limit of about 0.732 and the HDI method has an upper limit around 0.55.Based on the plot in part a the HDI approach is more appropriate because f(lambda) decreases and approaches 0 rapidly after 0.50. The more likely values are between 0 and 0.5 which is why the highest posterior density approach is better in this case. 

d) 

```{r}

qnorm(c(0,.95), -13, 2)

#finding highest density interval using hdi code 
set.seed(1000)
tst <- rnorm(100, -13, 2) #randomly chose 100 for n
hdi(tst)
hdi(tst, credMass=0.95)
```

```{r}
plot_normal(-13,2^2)
```

e) Next the middle 95% approach.

```{r}
qnorm(c(0.025, 0.975), -13, 2)
```


f) The highest poster density approach gave a credible interval that ranged from 16.84 to -8.88. The middle 95% approach gave a CI between -16.919 and -9.08. They are not the same, but they are very similar. I imagine if I had a chosen a different set.seed value I may be even closer between the HDI and middle 95% approach for the normal distribution. 

## Exercise  8.9
a) what is posterior probability for alternative pi > 0.4? 
```{r}

#find posterior probability that pi > 0.4

post_proba <- pbeta(0.40, 4,3)
plot_beta(4,3, mean = TRUE)
#This can be interpreted as there's roughly 17.92% posterior chance that the probability is *below* 0.40. The alternative can be found by doing 1-.1792 which is 0.8208. Looking at the plot below confirms the density of f(pi) is higher after 0.40 so the 0.8208 value makes sense. 
```

b) Calculate the posterior odds and interpret. 
```{r}
post_prob <- 0.8208
post_odds <- post_prob/(1-post_prob)
```

Post_odds = 4.58035 which means that pi is nearly 5 times more likely to be above 0.40 than to be below 0.40. 

c) Calculate and interpret prior odds. 
```{r}
#prior probability that pi is < 0.40
prior_proba <- pbeta(0.40, 1, 0.8)
prior_prob <-  1 - prior_proba #prior probability that pi is greater than 0.40 
prior_odds <- prior_prob/(1-prior_prob)
```

Prior odds are 1.9809 which means the odds of pi being above .4 are about twice that of pi being below 0.40. 

d) Calculate and interpret Bayes factor. 
```{r}
BFa <- post_odds/prior_odds
```

The posterior odds of our alternate hypothesis are about 2 times higher than the prior odds which means our confidence in the alternate hypothesis increased. 

e) The posterior probability (0.8208) and Bayes Factor (2.312) establish evidence in favor the the altnerate hypothesis that pi is greater than 0.40. 

# Corrected 
## Exercise 8.10 

a) Find posterior probability that mu < 5.2 for N(5,3^2)
```{r}
post_probb <- pnorm(5.2, 5, 3)
```

b) 
```{r}
post_oddb <- post_probb/(1-post_probb)
```

The post_odds of 1.112 means that mu is about 1 times more likely to be below 5.2 than to be above 5.2. 

c) 
```{r}
prior_probb <- pnorm(5.2, 10, 10)
prior_oddb <- prior_probb/(1-prior_probb)
```

Corrected: Prior odds are 0.4611 which means the odds of mu being below 5.2 are about half that of mu being above 5.2.
```{r}
plot_normal(5,3)
```

d)

```{r}
#finding Bayes Factor 

BFb <- post_oddb/prior_oddb
```

The Bayes Factor is 2.41188 which means posterior odds of our alternate hypothesis are about 2 times higher than the prior odds, therefore  our confidence in the alternate hypothesis (mu is less than 5.2) increased.

e) The posterior probability (0.526576) and Bayes Factor (2.41188) establish evidence in favor the the alternate hypothesis that mu is less than 5.2.  

#Exercise 8.14

a) The beta binomial is appropriate since we're interested in understanding a distribution of pi. 

b) I'm assuming that more people do believe in climate change than those that do not with mid level confidence so my prior is Beta(3,7) indicating more failures than successes for Y. 

c) Below are plots comparing my prior to that of the author's. 
```{r}
plot_beta(3,7, mean = TRUE)
plot_beta(1,2, mean = TRUE)
```

We both agree that beta > alpha meaning more people believe in climate change than not but the author's distribution shows more variability than mine which means they're less sure. 

d) First I'll take a look at the the data I'll use. 

```{r}
d <- pulse_of_the_nation
view(pulse_of_the_nation)
```

Next I'll create a set with just climate responses. Then I'll identify how many people responded that climate change is "Not Real At All." 

```{r}
d_climate_change <- d |>
  select(climate_change)  |> 
  drop_na()
view(d_climate_change)
```

```{r}
sum(d_climate_change$climate_change == 'Not Real At All')
```

From this we see Y = 150 i.e. 150 respondents out of 1000 answered that climate change is "Not Real At All."

e) 
```{r}
#finding posterior model of beta
plot_beta_binomial(alpha = 1, beta = 2, y = 150, n = 1000)
summarize_beta_binomial(alpha = 1, beta = 2, y =150, n = 1000)
```

Posterior is Beta(151, 852)

```{r}
#finding middle 95% posterior credible interval 
# 0.025th & 0.975th quantiles of the Beta(151,852) posterior
qbeta(c(0.025, 0.975), 151, 852)
```

There is a 95% posterior probability that somewhere between 12.91% and 17.33% of people don't believe in climate change. 

## Exercise 8.15

a) Based on the credible interval I would say that Ha: pi > 0.1 is more likely. 

b)
```{r}
post_probc <- pbeta(0.10, 151,852) #post_probc of pi being less than 0.1
post_probcc <- 1-post_probc #doing the complement to find prob of pi being greather than .1
```
This can be interpreted as there's roughly 99.99% posterior chance that the probability is *above* 0.10.

c) Calculate and interpret bayes factor. 

```{r}
post_oddc <- post_probcc/(1-post_probcc) #finding posterior odds
post_oddc
```

Next find prior odds 
```{r}
prior_prob <- pbeta(.1, 1, 2)
prior_probc <- 1-prior_prob
prior_probc
```

```{r}
prior_oddc <- prior_probc/(1-prior_probc)
prior_oddc
```
```{r}
BFc <- post_oddc/prior_oddc
BFc
```

d) The Bayes Factor is 750017.6 which means posterior odds of our alternate hypothesis are about 750017.6 times higher than the prior odds, therefore our confidence in the alternate hypothesis (pi is greater than .1) increased.

#Corrected 

## Exercise 8.16

a) 
```{r}
# define the model; this is set of instructions (could've written this in notepad) that will pass to stan through r 
climate_model <- "
  data {
    int<lower = 0, upper = 1000> Y;
  }
  parameters {
    real<lower = 0, upper = 1> pi;
  }
  model {
    Y ~ binomial(1000, pi);
    pi ~ beta(1, 2);
  }
"

# simulate the posterior
climate_sim <- stan(model_code = climate_model, data = list(Y = 150), 
                chains = 4, iter = 5000*2, seed = 80)


```

b) 
Produce and discuss trace plots, overlaid desnity plots, and autocorrelation plots. 
```{r}
# Parallel trace plots & density plots
mcmc_trace(climate_sim, pars = "pi", size = 0.5) + 
  xlab("iteration")
mcmc_dens_overlay(climate_sim, pars = "pi")

# Autocorrelation plot
mcmc_acf(climate_sim, pars = "pi")
```
The MCMC trace plot looks appropriate (like a fuzzy caterpillar so it is stable) and pi is between 12.5% and 17.5%.The density plot shows a peak somewhere between 0.14 and 0.16. The autocorrelation plots show agreement among the four chains. 

c) 
```{r}
# Markov chain diagnostics
rhat(climate_sim, pars = "pi")
neff_ratio(climate_sim, pars = "pi")
```

Rhat of 1 means that the simulation is extremely stable. If we multiply the neff by 20000 we get 6475.052 so this simulation is as effective as 6475 independent samples. 
```{r}
  0.3237526*20000 #multiplying by 20000 because four Markov chains each with 5000 iterations 
```

# Corrected 

## Exercise 8.17

a) Use MCMC simulation to approximate a middle 95% posterior credible interval for pi using tidy. 

```{r}

# MCMC posterior approximation; corrected to use direct calculation andd tidy()

tidy(climate_sim, conf.int = TRUE, conf.level = 0.95)

#direct calculation 
climate_chains_df <- as.data.frame(climate_sim, pars = "lp__", include = FALSE)
dim(climate_chains_df) #creating dataframe 

climate_chains_df |> #direct calculation
  summarize(
    low = quantile(pi, 0,25),
    up = quantile(pi, 0.975),
  )


```

```{r}
tidy(climate_sim, conf.int = TRUE, conf.level = 0.95) # A tibble: 1 × 5
```
```{r}
mcmc_areas(climate_sim, pars = "pi", prob = 0.95) #shade 95% a visual complement to the mcm estimate
```

b) approximate posterior that probability that pi > 0.10
```{r}
# store 4 chains in a data frame
climate_chains_df <- as.data.frame(climate_sim, pars = "lp__", include = FALSE)
dim(climate_chains_df)

# posterior summaries of pi 
climate_chains_df |> 
  summarize(post_mean = mean(pi), 
            post_median = median(pi),
            post_mode = sample_mode(pi),
            lower_95 = quantile(pi, 0.025),
            upper_95 = quantile(pi, 0.975))
```

```{r}
climate_chains_df |> 
  mutate(above = pi > 0.10) |> #pi values that are above 0.10
  tabyl(above)

climate_chains_df |> 
  mutate(below = pi < .10) |> 
  tabyl(below)
#interesting to see how tabyl returned the same percent of above and below .1

climate_chains_df |> 
  mutate(equal = pi > .12) |> #checked the approximate .95s CI values which makes more confident about my MCMC simulation 
  tabyl(equal)

climate_chains_df |> 
  mutate(equals = pi < .17) |> 
  tabyl(equals)
```

c) The approximations in part a and b shows a post mode and mean of about 0.15. The exact posterior mean found in 8.14 is about 0.15 so the approximation is very close to the actual. The confidence intervals are also very similar to each other (they mostly differ on third decimal).  

#Corrected 

## Exercise 8.18

a) 
```{r}
# Set the seed
set.seed(100)

# Predict a value of Y' for each pi value in the chain
climate_chains_df <- climate_chains_df |>  
  mutate(y_predict = rbinom(length(pi), size = 100, prob = pi))

# looking at the chains 
climate_chains_df |> 
  head(5)

```

Histogram of the model 
```{r}
# Plot the 20,000 predictions
ggplot(climate_chains_df, aes(x = y_predict)) + 
  stat_count()
```

b) 
```{r}
climate_chains_df |>  
  summarize(mean = mean(y_predict),
            lower_80 = quantile(y_predict, 0.1),
            upper_80 = quantile(y_predict, 0.9))
```
c) There's an 80% chance that Y' is between 10 and 20. 
```{r}
climate_chains_df |> 
  mutate(is = y_predict >= 20) |> #correction: using tabyl to find pi of Y = 20
  tabyl(is)


```
This shows that there's an .118 change that Y is greater than or equal to 20. 

# Corrected 

## Exercise 8.19 

a) 
The normal-normal is appropriate since we want to know the central tendency of a series of Y values.

b) My prior understanding is that mu is 200mm. We're given a range of 140 and 260 to use to find stdev. 

```{r}
p <- c(140,142, 145, 150, 155, 157, 158, 175, 177, 180, 182, 270, 276, 260)
sd(p) #standard deviation is several random values to estimate
```
Prior normal is N(200,48); though 48 seems high probably because I only used 14 values to estimate this sd. 

```{r}
#plotting this distribution 
data.frame(x = rnorm(10000, #dataframe with 1 column x and 10,000 pulls 
                     mean = 200, 
                     sd = 48)) |> 
             ggplot(aes(x =x )) + 
             geom_density() #good tool for checking priors 
```


c)
With help from Braulio, found the number of Adelie penguins their mean height and true sd. 
```{r}
penguins_bayes |> filter(species=="Adelie") |> 
  summarize(pen=n(), mean=mean(flipper_length_mm, na.rm=TRUE), sd=sd(flipper_length_mm, na.rm=TRUE)) #this is likelihood (what's observed in the data); prior is not affecting this at this state 
```
```{r}
data.frame(x = rnorm(10000, #dataframe with 1 column x and 10,000 pulls 
                     mean = 189.9536, 
                     sd = 6.539457)) |> 
             ggplot(aes(x =x )) + 
             geom_density() #notice that this is narrower and center is lower than prior estimate
```


```{r}
qnorm(c(0.025, 0.975), 189.9536, 6.539457) #this is just the 95% CI for the liklihood; notice that you haven't used data in this qnorm 
```

```{r}
summarize_normal_normal(mean = 200, #prior center of mass 
                        sd = 48, #prior stdev
                        sigma = 6.53, #data has sigma of 6.53
                        y_bar = 189, #data had this mu 
                        n = 152) #sample size; #variance is small because we're estimating credible interval of mu; not of the flipper lengths; this is dist. of possible mu 

plot_normal_normal(mean = 200, #prior center of mass 
                        sd = 48, #prior stdev
                        sigma = 6.53, #data has sigma of 6.53
                        y_bar = 189, #data had mu 
                        n = 152) #we're estimating one parameter mu; notice x-axis 
qnorm(c(0.025, 0.975), 189.0013, 0.5296207)
```


There is a 95% posterior chance that the mean is between 187.96 and 190.0393. 

# Corrected 

## Exercise 8.20 

a) Write a 2 sided hypothesis test 
H0: mu < 200
Ha: 200 < mu < 220

b) I would say that I would fail to reject the null hypothesis based on the credible interval from 8.19 which was between 187.96 and 190.0393; this strongly suggests that mu will not exceed 200. 

c) 
```{r}
pen_model <- "
data {
    vector[146] Y; 
} 
parameters {
    real mu;
} 
model {
   Y ~ normal(mu, 6.5);
   mu ~ normal(200,7 );
}
"
d <- list(Y = penguins_bayes |> #making a list 
  filter(species == "Adelie") |> drop_na() |> #filtering for Adelie species  
  pull(flipper_length_mm)) #we're just interested in vector of flipper lengths 

pen_sim <- stan(model_code = pen_model, data = d, chains = 4, iter = 5000)


```


## Exercise 8.21 

a) The Gamma-poisson is appropriate since we're interested in lambda and a rate. 

b) My prior is Gamma(1,2); this is based on wanting the model to be right skewed to indicate the lower rate (2 per 100) is more likely than a higher rate. I used view(loons) to see the range of possible rates is between 0 and 5. 

```{r}
view(loons)
```

c) There are 18 data points. The average loon count is 1.5.
```{r}
x <- c(0, 5, 0, 0, 0, 2, 0, 2, 1, 1, 0, 4, 2, 0, 3, 3, 1, 3) #create vector 
result.mean <- mean(x) #find mean 
print(result.mean) #viewing results. This section of code was inspired by https://www.tutorialspoint.com/r/r_mean_median_mode.htm
```

d) Find middle 95% posterior credible interval for lambda. 

```{r}
# 0.025th & 0.975th quantiles of the Gamma(18,92) posterior
qgamma(c(0.025, 0.975), 1, 1.5) #used average rate from part c for rate. 

```

## Exercise 8.22

a) The hypotheses are H0: lambda is greater than or equal to 1 and Ha: lambda <1

b) Based on the credible interval from 8.21 and the data, I would expect the that both H0 and Ha are plausible and would need more testing to be decisive. 

c) 
```{r}
qgamma(c(0.025, 0.975), 1, 1.5)
```
d) I would interpret this as most lambda values are between 0 and 3. 

## Exercise 8.23
```{r}
# Step 1: Define  the model
loongamma_model <- "
  data {
    int<lower = 0> Y[18];
  }
  parameters {
    real<lower = 0> lambda;
  }
  model {
    Y ~ poisson(lambda);
    lambda ~ gamma(1, 1.5);
  }
  "
# Step 2: simulate the posterior
loon_sim <- stan(model_code = loongamma_model, data = list(Y = c(0, 5, 0, 0, 0, 2, 0, 2, 1, 1, 0, 4, 2, 0, 3, 3, 1, 3)), 
               chains = 4, iter = 5000*2, seed = 500)

#trace and density plots 
mcmc_trace(loon_sim, pars = "lambda", size = 0.3)

mcmc_dens(loon_sim, pars = "lambda") + 
  yaxis_text(TRUE) + 
  ylab("density")
```


d) Based on the simulation, the MCMC looks stable.  


## Exercise 8.24

```{r}

## still need to correct this one.
# store 4 chains in a data frame

loons_chains_df <- as.data.frame(loon_sim, pars = "lp__", include = FALSE)
dim(loons_chains_df)

# Set the seed
set.seed(100)



# looking at the chains 
loons_chains_df |> 
  head(5)



## This set of code isn't working and had trouble with 8.19 - 8.24. 
```

