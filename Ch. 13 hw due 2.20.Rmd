---
title: "Ch. 13 HW due 2.20.22"
author: "Brenda Onyango"
date: "2/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 13


### Errors and residuals 

Y = 2 + 3x + $\epsilon$

a. find error for observation A. 

9 = 2 + 3(2) + $\epsilon$

$\epsilon$ = 1

b. Y = $\beta_0$ + $\beta_1$*X + $\epsilon$

find the residual for Observation A 
9 = 1.9 + 3.1(2) + $\epsilon$

$\epsilon$ = 0.9 = residual since the residual is the difference between the prediction we make with our fitted line and the actual value. The "error term" represents the residual when we're using a prediction line. 

### 2: Writing the regression equation 

2) I would use:

Y = B_0 + B_1*X + B_2*A + B_3*B + e

Including coefficients for A and B closes the backdoors X <- A -> Y and X <- B -> Y. C is included in e. B_0 is a constant/intercept. 

### 3: Coefficients 

3) 
a. The coefficient that multiplies a variable is a slope, so a one-unit increase in X results in a 3 increase in Y. 

b. 
```{r}
pnorm(q = (3/1.3)) #using pnorm and z-score to find the percentiles. Z score is score - mean/stdev and in this case we can use 0 as mean and the standard error as stdev. 
```

The estimate of 3 is in the 98.95th percentile of the normal distribution. 

```{r}
a  <- (100 - 98.95)*2 
a
```

 I multiplied by 2 above to do a two-tailed test. If our alpha = 0.05 then we reject the null since 2.09% is less than 5% and the estimate is statistically significant from 0. 

c. To say that the coefficient is statistically significant from 0 is to say that 0 is an unlikely value for the coefficient. It doesn't mean that 3 is a true value or that the effect is meaningful. 

### 4: Women's work 

1) annual hours worked = years of education + constant
2) annual hours worked = #ofchildren + constant
3) annual hours worked = years of education +  #ofchildren + constant

are a basic representation of what the columns in the table represent. 

a. Adjusting for number of children a one-unit increase of years of education is associated with 76.185 increase in annual hours worked. 

b. When not adjusting for years of education, the standard error on the "children under 5" is 19.693. 

c. For a woman with 0 children and 0 years of education, the predicted number of hours worked annually is the constant or intercept which is 306.553. 

d. 3382 observations are used in each regression. 

e. The coefficient on "children under 5" is statistically significant from 0 at the p<0.01 or the 99% level as indicated by the three asterisks. This also means that p is less than 0.05 so the coefficient is also statistically signicant from 0 at the 95% level. 


### 5 

annualhoursworked = 10.145 + 11.230YearsEducation - 1.581YearsEducation^2

a. A one-year increase in YearsEducation is associated with a (110.230 - 3.162YearsEducation) hour change in annual hours worked. 

b. A one year increase in YearsEducation will increase hours worked by 59.638 if the current level of education is 16. 

```{r}
(110.230) - (3.162 * (16))
```

c. 
```{r}
YearsEducation <- c(0,1,2,3,4,5, 6, 7, 8, 9,10,10.145, 11, 12, 13)
annualhoursworked <-  10.145 + 11.230*YearsEducation - 1.581*(YearsEducation^2)
annualhoursworked
```

```{r}
plot(annualhoursworked)
```

d. Mathematically, the squared term has a negative coefficient which tells us that the parabola is an upside-down u or getting less positive with higher x-values. The plot above also shows that the relationship between YearsEducation and AnnualHoursWorked are getting less positive for higher values of YearsEducation. 

d. One reason to not include more polynomial terms is to avoid overfitting. More polynomials can lead to overfitting if a model tries to fit noise and ends up performing worse than a model with fewer polynomials. 

### 6: Binary and categorical variables 

a. The coefficient on Homeowner is 50.174 which means that on average, homeowners work that many more hours annually than non-owners. 

b. The coefficient for "4 children under 5" = -923.904 and the coefficient for "3 children under 5" = -773.412. 

The difference: 
```{r}
fourchildren <- -923.904
threechildren <- -773.412

fourchildren - threechildren 
```

People with 4 children under 5 work on average 150.492 fewer hours than people with 3 children under age 5. 

c. From this table we can't tell if there's a statistical significance between having 2 children and having 3 children. We need to perform a joint F test and could do so using the linearHypothesis in R. 

### 7: log transformation 

a. In Model 1 a one-unit increase in education is associated with an 110.073 increase in annual hours worked at the 99% level when adjusting for homeowner. 

b. Assuming that annual hours worked is positively correlated with annual earnings then yes, annual earnings rise more quickly for homeowning families than nonhomeowning families. The regression table shows us that on average homeowners work 682.992 more hours (*note table shows hour not earnings*) annually than non-owners when adjusting for education. The difference between homeowners and nonhomeowners regarding hours worked is statistically significant at the 99% level (which includes the 95% level).

c. Homeowner x Education is an interaction term meaning that the effect of homeowner status on annual hours worked is moderated by education.  The effect of being a homeowner on annual hours worked is 53.994 weaker when education is increased by one unit. 

d. In model 2, when adjusting for homeowner, for every one-unit increase in education, annual hours worked increases by about 6.7%. 

e. In model 3, log(Education) has a coefficient of 832.347. This tells us that for a 1% increase in education, annual hours worked will increase by about 8.323 hours. 8.32 was found by doing (832.347/100). It's important to notice if both predictor and outcome variables are log transformed. 

To use 10% we can say a 10% increase in education, is associated with a 83.2344 increase in annual hours worked. 

f. The sample sizes may vary between models because the log transformations can't take log(0). 

### Autocorrelation & heteroskedasticity (optional)

8) b. Of these options, b seems to be the most accurate definition as it describes temporal autocorrelation. 

9) a and b. I think c doesn't make sense to determine issues with standard errors with a term that's not in the model. R^2 is about the quality of the model in terms of whether our DGP was more complete than not so d is eliminated. That elaves a,b, and e. A and b 

### 10: Polls and Sample weights 

The pollsters can use sample weights to address this problem. Like the chapter discussed, the pollsters could compare the proportion of people with a given attribute to the population value and find the probabilities of people with different attributes being included. Each individual would be weighed by the *inverse* of the probability so that people less likely to be included are counted for more. This makes the sample more representative of the population. 

### 11: non-classical measurement error 

11) a 

Non-classical measurement error is related to the true value. In a the true value is harder to access because it's a sensitive topic. I'm assuming c is using data from an archive and that the data was generated in a validated way so is not a non-classical error.


## Chapter 13: Coding HW

1) Confirming that I've uploaded the dengue csv file and named in dengue. 

```{r}
library(readr)
library(tibble)
```

```{r}
dengue <- read_csv("dengue.csv")
print(dengue)
```

2) run OLS regression using average humidity as x and dengue in area as y. 

```{r}

library(modelsummary)
library(tidyverse)
```



```{r}

m2 <- lm(NoYes ~ humid, data = dengue) #OLS using average humidity as predictor

```

```{r}
msummary(list(m2))
stars =TRUE
```

Let's plot the data to better visualize what's happening. 

```{r}
plot( x = dengue$humid,
      y =dengue$NoYes,
      xlab = "humidity",
      ylab = "dengue",
      pch = 20)

plot (x = dengue$humid,
      y = log(dengue$NoYes), #plotting the data with log transform of NoYes
  main = "scatter with logy",
              pch = 22)
```

3) Two sentences: A one unit increase in humidity as measured by average vapor density is associated with a 0.05 increase in the value of YesNo*. The intercept -0.416 is the value of YesNo when humidity is 0. 

YesNo only has 2 values so the above interpretation of the coefficient and the intercept can be improved by log transforming YesNo as I did in the second plot. 
Below is a regression with YesNo log transformed. Now the intercept is no longer negative; this makes more sense since we can't have negative dengue; the lowest the incidence/prevalence of dengue can be is 0. 

```{r}
dengue <- dengue |> mutate(newcol = 1 + (log(NoYes))) #adding 1 to log transformation since log(0) is undefined. 
```

I tried running a regression with newcol ~ humid which works with .rmd but not when I knit. In later exercises I correctly run a logit. The above chunk was good practice adding a column to a dataset. The model I created with mlog <- lm(NoYes ~humid, data = dengue) had the following interpretation:  For every unit increase of humidity, The value of NoYes increases by 0.034. 0.712 is the value of NoYes when humidity is 0; this makes more sense since it's not negative. 


4. 
```{r}
summary(dengue$humid) #getting summary statistics of humdity 
IQR(dengue$humid, na.rm = TRUE)
```

The summary stats show that the lowest possible value of humidity in this set is 0.6714. This also makes sense realistically given that humidity can never be 0 in Earth's atmosphere.  (source: https://www.chicagotribune.com/news/ct-xpm-2011-12-16-ct-wea-1216-asktom-20111216-story.html). 

In both the untransformed and log transformed regressions we now can calculate a new minimum value for NoYes.The regression with NoYes logtransformed has a minimum value of NoYes that is more realistic; for the non-transformed regression we still have a value for NoYes below 0 which is not possible.  

```{r}
a <- -0.416 + 0.05*0.6714
b <- 0.712 + 0.034*0.6714
a
b

```

### 5: Controlling for temperature

Add a control for temperature, rerun regression, and show results. 

```{r}
m5 <- lm(NoYes ~ humid + temp, data = dengue)
msummary(list(m5),
stars =TRUE)
```


# Binary dependent variable 

Rerun regression from 5 as a logit model and report marginal effects of both slope coefficients. 

```{r}
library(margins) #loading library used in chapter 
```

```{r}
m6 <- glm(NoYes ~ humid + temp, data = dengue, family =binomial(link='logit'))

msummary(m6,
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01)) #viewing results


```

```{r}
#marginal effects of coefficients for humidity and temperature
m6 |> margins(variables = 'humid') |> summary()
m6 |> margins(variables = 'temp') |> summary() #when I tried putting humid and temp in one term I got subscript out of bounds error but it ran with them separate. 
```

The marginal effect for the coefficient on humid is 0.0317 and the marginal effect for the coefficient on temp is 0.0042.

### Relationship b/w temp and humidity 

Run an OLS regression of humidity on temperature. Rerun the model with heteroskedasticity-robust standard errors. 

```{r}
denguehum <- dengue |> filter(!is.na(dengue$humid))
m7 <- lm(humid ~ temp, data = denguehum) #question says regression of humidity on temperature so humid is Y(dependent) and temp is x 
msummary(list(m7),
stars =TRUE)
```

Calculate the residuals of that regression, and then make a plot that will let you evaluate whether there is likely heteroskedasticity in the model. 

```{r}
#calculate residuals
res <- resid(m7)
```


```{r}
plot(denguehum$temp, res,
     ylab = "Residuals of humid on temp", xlab = "Temperature",
     main = "Heteroskedasticity Check")
```

Looking at the above plot we see that as temperature increases,  variation in the error term increases. This means that there is a failure of the "independent and identically distributed" assumption for the error term and so there is likely heteroskedasticity. 

Rerun the model with heteroskedasticity-robust standard errors. Show both models, and say whether you think there is heteroskedasticity Save the model as `m7`, and `m7b` if you rerun with robust standard errors.

```{r}
library(sandwich) #loading packages ch. 13 used for robust SEs
library(fixest)
```

```{r}
msummary(m7, vcov = 'robust', 
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01)) #used the same regression object and did robust SEs inside msummary
```


Seeing that the intercept and temp coefficients are the same in the model with robust SEs now makes me think that there likely isn't heteroskedasticity. 


### 8: Robust standard errors 

Run the model from question 7 again (with heteroskedasticity-robust standard errors), but this time use the logarithm of humidity in place of humidity. Add a sentence interpreting the coefficient on temperature. 


```{r}
m8 <- lm(log(humid) ~ temp, data = denguehum)
modelsummary(m8, vcov = 'robust', 
         stars = c('*' = 0.1, '**' = 0.05, '***' = 0.01))
```

For every one-unit increase in temperature, humidity increases by [exp(0.056) - 1]* 100 = 13.76%. 

```{r}
plot(dengue$temp, dengue$humid) #taking a look at untransformed temperature and humidity data to confirm that saying temp and humid have positive association is sensible. 
```

### Bonus: why log humidity and keep temp linear?

Looking at the plot above we see that there may some outliers that doing a log transformation for humidity would help to reduce. Another reason, and the more likely one, is that the line of best fit seems like it would be curve upwards as temperature increases. The relationship between temperature and humidity might be better explained with a model that shows a multiplicative relationship. 