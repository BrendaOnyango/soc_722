---
title: "HW 10: Bayesian Regression"
author: "Brenda Onyango"
date: "10/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exercise 9.1

a) A normal prior is reasonable for B0 and B1 because these can take values around a center value (positive and negative or even zero).  

b) A normal prior isn't reasonable for $\sigma$ because sigma can't be negative. 

c) The difference between weakly informative priors and vague priors is that weakly informative priors are more focused in that they reflect uncertainty  
across a range of *sensible* parameter values, whereas vague priors put weight on non-sensible parameter values. With weakly informative priors, chains don't have to spend time exploring non-sensible parameter values. 

## Exercise 9.2

Predictor = X and response variable = Y

a) X = arm length, Y = height 
b) X = distance b/w home and work, Y = person's carbon footprint
c) X = age, Y = child's vocabulary level
d) X = person's sleep habits, Y = reaction time 

Notice these have an order i.e. a person's carbon footprint can't happen before they commute.

## Exercise 9.3 

Y = B0 + B1X where B0 is y-intercept (what typical Y value is when and X = 0) and B1 is slope. X indicates predictor.

a) Y_kangheight = B_0 + X_ageB_1 Here B_0 is the height of the kangaroo at 0 months old or when it's born and B_1 is how much the kangaroo grows for each increase in month. B_1 is positive.

b) Y_Githubfollowers = B_0 + X_commitsB_1 Here B_0 is the number of followers when the data scientists has 0 Github commits and B_1 is how many followers are added for each increase in Github commits. Assuming B_1 is positive. 

c) Y_visitors = B_0 + X_rainfallB_1 B_0 is the number of visitors to a local park when there are 0 inches of rainfall. B_1 is the decrease in visitors for each additional inches of rainfall. B_1 is negative.

d) Y_netflix = B_0 + X_sleepB_1 B_0 is the daily hours of Netflix a person watches when they have 0 hours of sleep. B_1 is the decrease in netflix hours for each increase in the hours of sleep. B_1 is negative. 

## Exercise 9.4 

$\sigma$ tells us about the spread of data in our sample. More spread or a higher standard deviation means that the relationship between X and Y is weaker. Less spread or lower variation means the relationship between X and Y is stronger. 


```{r}
# Load packages
library(bayesrules)
library(tidyverse)
library(rstan)
library(rstanarm)
library(bayesplot)
library(tidybayes)
library(janitor)
library(broom.mixed)
```


## Exercise 9.5 

a) X = predictor = person's age in years, Y = orange juice consumption in gallons

b&c) data: $Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(m0, s^2)

B1 ~ N(m1, s1^2)

$\sigma$ ~ Exp(l)

d)  The unknown parameters are BO, B1, and $\sigma$. B1 can be 0 if we include infants under the age of 1 but a more realistic value is between 1-78 years of age with an average of 38 and sd of 13 years; B0 has to be positive (can't drink negative amount of orange juice). $\sigma$ has to be positive.  

e) Let's assume that the capacity to drink OJ increases as we increase in age because of growing stomach capacity. Drinking OJ could have a normal distribution where younger and older people drink less than middle aged people.

Let's say that the average amount of gallons is 20 (around 320 cups a year). let's say the amount of orange juice people drink per year has standard deviation of 2 gallons. 

For the age assumptions I used average age, life expectancy, and sd from google searches. For the orange juice priors I based on my personal habits of drinking between 0.5-1.5 cups of OJ in a day and multiplied by 365 to get around 20 gallons a year.

```{r}

plot_normal(mean = 20, sd = 2) + #average of .05 gallons of oj for B1
  labs(x = "beta_0c", y = "pdf")
plot_normal(mean = 38, sd = 13) + #average age and sd of 13 years 
  labs(x = "beta_1", y = "pdf")
plot_gamma(shape = 1, rate = 0.07692307692) + #exponential model of sd 1/13 gives l of 0.07692307692 
  labs(x = "sigma", y = "pdf")
 
```
Plugging in the tuned priors we can specificy the Bayesian regression model like this: 
$Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(20, 4)

B1 ~ N(38, 169)

$\sigma$ ~ Exp(0.0769)

# Exercise 9.6

a) X = predictor = today's high temperature, Y = tomorrow's high temperature. 

b &c) data: $Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(m0, s^2)

B1 ~ N(m1, s1^2)

$\sigma$ ~ Exp(l)

d) TThe unknown parameters are BO, B1, and $\sigma$. B1 can between 0 and 56 degrees celcius (highest temp ever recorded was 134).  B0 has to be positive and also in the same range as B1 since they're both temperatures.  $\sigma$ has to be positive.

e)  I'll assume it's fall so the average of today's temperature and tomorrow's is 15 degrees Celsius. I'll assume a sdev of 3 degrees Celsius. I'll use these values to build a model. 

```{r}
plot_normal(mean = 15, sd = 3) + #average of 15 degrees for B1
  labs(x = "beta_0c", y = "pdf")
plot_normal(mean = 15, sd = 3) + #average temp and sd of 3 degrees celsius 
  labs(x = "beta_1", y = "pdf")
plot_gamma(shape = 1, rate = 0.333333) + #exponential model of sd 1/3 gives l of 0.333
  labs(x = "sigma", y = "pdf")
```


My tuned model is now: data: $Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(15, 9)

B1 ~ N(15, 9)

$\sigma$ ~ Exp(0.333)

# Exercise 9.7 

a) False; MCMC is used to *approximate* the posterior. 

b) True. 

# Exercise 9.8

Specify appropriate stan_glm() syntax for simulating the Normal regression model using 4 chains each of length 10000 (don't run the code). 

a) X = age; Y = height; dataset name: bunnies 


bunnies_model <- stan_glm(height ~ age, data = bunnies,
                       family = gaussian,
                       prior_intercept = normal(m0, s0^2),
                       prior = normal(m1, s1^2), 
                       prior_aux = exponential(1/s1),
                       chains = 4, iter = 5000*2, seed = 800)
                       
b) X = Snaps, Y = Clicks; dataset = songs 

songs_model <- stan_glm(clicks ~ snaps, data = songs,
                       family = gaussian,
                       prior_intercept = normal(m0, s0^2),
                       prior = normal(m1, s1^2), 
                       prior_aux = exponential(1/s1),
                       chains = 4, iter = 5000*2, seed = 801)
                       
c) X = age; Y = happiness; dataset = dogs 

happy_model <- stan_glm(happiness ~ age, data = dogs,
                       family = gaussian,
                       prior_intercept = normal(m0, s0^2),
                       prior = normal(m1, s1^2), 
                       prior_aux = exponential(1/s1),
                       chains = 4, iter = 5000*2, seed = 800)

## Exercise 9.9 

a) 
$Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(5000,2000) based on using range rule (max-min/4) we can say sdev is 2000. 

B1 ~ N(10, 5) based on range rule sdev is 20/4 so about 5

$\sigma$ ~ Exp(0.2)

b) 
```{r}
# Load and plot data
data(bikes)
ggplot(bikes, aes(x = humidity, y = rides)) +#taking a look at rides and humidity 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```

```{r}

#simulating the prior 
bike_modelp <- stan_glm(rides ~ humidity, data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 2000),
                       prior = normal(10, 5), 
                       prior_aux = exponential(0.2),
                       prior_PD = TRUE, #Including instructions to find prior 
                       chains = 5, iter = 4000*2, seed = 855)

#visualizing the chains and density plot

# Trace plots of parallel chains
mcmc_trace(bike_modelp, size = 0.1)

# Density plots of parallel chains
mcmc_dens_overlay(bike_modelp)

# Prior summary statistics
tidy(bike_modelp, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)


```

```{r}
#neff and rhat

neff_ratio(bike_modelp)

rhat(bike_modelp)
```
The r-hat is around 1 which suggests chains are stable. 

c) 
```{r}
# 100 simulated model lines
bikes |> 
  add_fitted_draws(bike_modelp, n = 100) |> 
  ggplot(aes(x = humidity, y = rides)) + #plotting humidity and rides 
    geom_line(aes(y = .value, group = .draw), alpha = 0.15) + 
    geom_point(data = bikes, size = 0.05)

# Simulate four sets of data
bikes |> 
  add_predicted_draws(bike_modelp, n = 4) |> 
  ggplot(aes(x = humidity, y = rides)) + #changed froom textbook to humidity 
    geom_point(aes(y = .prediction, group = .draw), size = 0.2) + 
    facet_wrap(~ .draw)
```


d) These plots show different, positive relationships which indicates posterior uncertainty about the relationship between humidity and rides. The plot I made in part b shows a weak negative association between humidity and rides. 

## Exercise 9.10 

a) This plot I made earlier shows a weak negative association between humidity and rides.

```{r}
# Load and plot data
data(bikes)
ggplot(bikes, aes(x = humidity, y = rides)) +#taking a look at rides and humidity 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```


b) Simple normal regression is reasonable to use because we have BO, B1, and sigma as unknowns. The only issue with using simple normal regression in this case is that there is a a lot of variability in the data that leads to uncertainty. 

## Exercise 9.11

```{r}

#a)
#simulating the posterior 
bike_model <- stan_glm(rides ~ humidity, data = bikes,
                       family = gaussian,
                       prior_intercept = normal(5000, 2000),
                       prior = normal(10, 5), 
                       prior_aux = exponential(0.2),
                       chains = 5, iter = 4000*2, seed = 850)

#b)

# Trace plots of parallel chains
mcmc_trace(bike_model, size = 0.1)

# Density plots of parallel chains
mcmc_dens_overlay(bike_model)

# Posterior summary statistics
tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)


```

```{r}
#neff and rhat

neff_ratio(bike_model)

rhat(bike_model)
```


The MCMC chains look like fuzzy caterpillars (stable). Rhat is close to 1 meaning chains are stable. The MCMC approximation shows a humidity CI between -4.755891 and 2.907006; the prior had a CI of 3.62-16.488. The posterior relationship is 3543.143 + -.930773X. For every increase in humidity we expect riders to decrease by about 1.Slope can range between -4.755891 and 2.907006. 


c) 
```{r}

#posterior simulation 
bike_default_post <- update(bike_modelp, prior_PD = FALSE)
# 100 post model lines 
bikes |> 
  add_fitted_draws(bike_default_post, n = 100) |> 
  ggplot(aes(x = humidity, y = rides)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)
```
Compared to the prior, the posterior model has many more lines with a negative slope. Both the prior and posterior show a lot of variability among the lines. 

## Exercise 9.12

```{r}

#a)
# Posterior summary statistics
tidy(bike_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.95) #.95 credible interval
```

b) The posterior median value of sigma (1281) means that values of Y (riders) will vary around the mean by this standard deviation. 

c) The 95% posteior credible interval for humidity is between -6.859 and 4.92 meaning most values of humidity will be in this interval for the most probable lines. 

d) Yes, we have a negative estimate for humidity and negative values in the 95% CI. 

## Exercise 9.13

Without using the posterior_predict function model (1) the posterior model for the typical number of riders on 90% humidity days; and (2)
the posterior predictive model for the number of riders tomorrow.

a)
```{r}

bike_model_df <- as.data.frame(bike_model) #storing 4 chains from posterior model as data frame
first_set <- head(bike_model_df, 1)
first_set
```
mu = BO^1 + B1^1X = 3729.559 + -2.95844X 

```{r}

mu <- first_set$'(Intercept)' + first_set$humidity * .90 #finding average of mu when humidity is .9


```

Mu is 3726.8960

```{r}
set.seed(853)
y_new <- rnorm(1, mean = mu, sd = first_set$sigma)
mu
```

y_new is 3264.8866 Notice this is below average rides on days with humidity of .90.

```{r}
# Predict rides for each parameter set in the chain
set.seed(853)
predict_90 <- bike_model_df |>  
  mutate(mu = `(Intercept)` + (humidity*.90),
         y_new = rnorm(20000, mean = mu, sd = sigma))

#see predictions 
head(predict_90, 5)
```

```{r}
# Construct 95% posterior credible intervals
predict_90 |> 
  summarize(lower_mu = quantile(mu, 0.025),
            upper_mu = quantile(mu, 0.975),
            lower_new = quantile(y_new, 0.025),
            upper_new = quantile(y_new, 0.975))
```

Used view to see what the variable for tomorrow (Sunday) is and it's sun. Built a posterior model for Sunday. 

```{r}
#see how often Sunday appears in data 

bikes |> filter(day_of_week=="Sun") |> 
  summarize(Sun=n())

#create dataset with just Sunday data
sunday_rides <- bikes |> 
  filter(day_of_week == "Sun")
```


#build posterior model for Sunday
First I'll take a look at rides on Sunday. 

```{r}
# Load and plot data
data(sunday_rides)
ggplot(sunday_rides, aes(x = day_of_week, y = rides)) +#taking a look at rides and day_of_week
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)

#will also take a look at data without Sunday filter
# Load and plot data
data(bikes)
ggplot(bikes, aes(x = day_of_week, y = rides)) +#taking a look at rides and humidity 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```



```{r}

#a)
#simulating the posterior 
sun_model <- stan_glm(rides ~ humidity, data = sunday_rides, #using sunday only data
                       family = gaussian,
                       prior_intercept = normal(5000, 2000), #using same prior
                       prior = normal(10, 5), 
                       prior_aux = exponential(0.2),
                       chains = 5, iter = 4000*2, seed = 830)
#posterior simulation 
sunday_post <- update(sun_model, prior_PD = FALSE)

# 5 post model lines 
sunday_rides |> 
  add_fitted_draws(sunday_post, n = 5) |> 
  ggplot(aes(x = day_of_week, y = rides)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)
sunday_df <- as.data.frame(sun_model)

#storing 4 chains from posterior model as data frame
set.seed(30)
second_set <- head(sunday_df, 1)
second_set
```

Based on these 4 chains mu_sun = 2272.662 + 10.96X

```{r}
mu_sun <- second_set$'(Intercept)'+second_set$humidity*.90 #finding average of mu when humidity is 90 on Sunday
mu_sun
```

```{r}
y_newsun <- rnorm(1, mean = mu, sd = second_set$sigma)
mu
```

```{r}
# Predict rides for each parameter set in the chain
set.seed(25)
predict_90v2 <- sunday_df |>  
  mutate(mu = `(Intercept)` + (humidity*.90),
         y_newsun = rnorm(20000, mean = mu, sd = sigma))

#see predictions 
head(predict_90v2, 5)
```

b) 
```{r}
# Trace plots of parallel chains
mcmc_trace(bike_model, size = 0.1)

# Density plots of parallel chains for first posterior of .9 humidity
mcmc_dens_overlay(bike_model)

# Trace plots of parallel chains for Sunday's posterior
mcmc_trace(sun_model, size = 0.1)

# Density plots of parallel chains
mcmc_dens_overlay(sun_model)
```


The density plots for both posteriors look stable.

c) 
```{r}
# Construct 80% posterior credible intervals for Sunday data
predict_90v2 |> 
  summarize(lower_mu = quantile(mu, 0.20),
            upper_mu = quantile(mu, 1),
            lower_new = quantile(y_new, 0.20),
            upper_new = quantile(y_new, 1.0))

#using tidy to find credible interval for tomorrow
tidy(sun_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)

```

The 80% interval for humidity (the slope) means humidity can be anywhere between 3.22 to 12.67.The same as for the most credible values of the intercept and sigma. 

d) Use posterior predict for tomorrow's riders. 

```{r}
set.seed(222)
fast_prediction <- 
  posterior_predict(sun_model, newdata = data.frame(humidity = 0.90))

# Construct a 80% posterior credible interval
posterior_interval(fast_prediction, prob = 0.80)

 
# Plot the approximate predictive model
mcmc_dens(fast_prediction) + 
  xlab("predicted ridership on a Sunday")
```

The posterior_predict shows an 80% credible interval for the number of riders to be between 1466 and 3293 on a Sunday when humidity is 90%. 

## Exercise 9.14

a) I would guess if it's too windy it might be dangerous to ride a bike so I would guess there would be a negative relationship between windspeed and biking. 


```{r}
data(bikes)
ggplot(bikes, aes(x = windspeed, y = rides)) + #taking a look at rides and windspeed to think of good priors 
  geom_point(size = 0.5) + 
  geom_smooth(method = "lm", se = FALSE)
```



b) $Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(6500,1625) based on using plausible (max-min/4) we can say sdev is approx. (6500-0/4)

B1 ~ N(40, 10) based on range rule sdev is 40/4 so about 10

$\sigma$ ~ Exp(0..10) found by 1/stdev of B1

```{r}
prior_summary(bike_model)
```

b) 
```{r}

wind_model_prior <- stan_glm(
  rides ~ windspeed, data = bikes, 
  family = gaussian,
  prior_intercept = normal(6500, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = TRUE,
  chains = 4, iter = 5000*2, seed = 840)
# 100 prior model lines with wind speed
bikes |> 
  add_fitted_draws(wind_model_prior, n = 100) |> 
  ggplot(aes(x = windspeed, y = rides)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)

# 4 prior simulated datasets
set.seed(30)
bikes |> 
  add_predicted_draws(wind_model_prior, n = 4) |> 
  ggplot(aes(x = windspeed, y = rides)) +
    geom_point(aes(y = .prediction, group = .draw)) + 
    facet_wrap(~ .draw)
```

Many of the 100 lines do reflect our prior understanding that rides decrease as windspeed increases, but there is variability. Half the datasets look similar and have a negative relationship. 

d) I made a plot in part a to help me think about the priors and it is consistent with how I understood the relationship between wind and bike speed. 

## Exercise 9.15

a) Make a posterior analysis. 
```{r}

wind_model <- stan_glm(
  rides ~ windspeed, data = bikes, 
  family = gaussian,
  prior_intercept = normal(6500, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  chains = 4, iter = 5000*2, seed = 840)
set.seed(222)
fast_prediction_wind <- 
  posterior_predict(wind_model, newdata = data.frame(windspeed = 20))

# Construct a 80% posterior credible interval
posterior_interval(fast_prediction_wind, prob = 0.80)

 
# Plot the approximate predictive model
mcmc_dens(fast_prediction) + 
  xlab("predicted ridership by windspeed")

tidy(wind_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```

The posterior understanding matches my prior understanding that windspeed and rides is negative. 

## Exercise 9.16

a) simulating the prior.

```{r}
pen_model_prior <- stan_glm(
  flipper_length_mm ~ bill_length_mm, data = penguins_bayes, 
  family = gaussian,
  prior_intercept = normal(45, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = TRUE, #making sure it's the prior 
  chains = 4, iter = 5000*2, seed = 83)

# 100 prior model lines with wind speed
penguins_bayes |> drop_na() |> #added drop_na() so code could run
  add_fitted_draws(pen_model_prior, n = 100) |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)
```


b) 
```{r}
prior_summary(pen_model_prior)
```

Using information from above below is the written prior model 
 $Y_{i}$|$B_O$,$B_1$, $\sigma$  ~ N($u_i$, $\sigma$^2) with $u_i$ = $B_O$ + $B_1$$X_i$

B0 ~ N(45,35) 

B1 ~ N(0, 6.4) based on range rule sdev is 40/4 so about 10

$\sigma$ ~ Exp(0.071)

c) 
```{r}
# 100 prior model lines
penguins_bayes |> drop_na() |> 
  add_fitted_draws(pen_model_prior, n = 100) |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)

# 4 prior simulated datasets
set.seed(40)
penguins_bayes |> drop_na() |> #added drop_na()
  add_predicted_draws(pen_model_prior, n = 4) |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
    geom_point(aes(y = .prediction, group = .draw)) + 
    facet_wrap(~ .draw)
```


d) My understanding is that as bill length increases flipper length decreases (3/4 of the plots show a negative slope). The line simulations show both positive and negative slopes. 

## Exercise 9.17

a) 
```{r}
data("penguins_bayes")
ggplot(penguins_bayes, aes(x = bill_length_mm, y = flipper_length_mm)) +
  geom_point(size = 0.1) + 
  geom_line(method = "lm", se = FALSE)

```


b) I would say based on 9.16 a simple normal regression may not work for this data since the prior lines simulation shows the data may behave more like a parabola than a line. 

## Exercise 9.18

a) 

```{r}
pen_model <- stan_glm(
  flipper_length_mm ~ bill_length_mm, data = penguins_bayes, 
  family = gaussian,
  prior_intercept = normal(51, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = FALSE,
  chains = 4, iter = 5000*2, seed = 83)

# 100 posterior model lines with wind speed
penguins_bayes |> drop_na() |> #added drop_na() so code could run
  add_fitted_draws(pen_model, n = 100) |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)
```

```{r}
view(penguins_bayes) #looking for pablo species in dataset
```



```{r}

#creating Pablo dataset
Pablo <- penguins_bayes |> 
  filter(species == "Gentoo") #used Gentoo species since Pablo isn't in the dataset

pablo_model <- stan_glm(
  flipper_length_mm ~ bill_length_mm, data = Pablo, 
  family = gaussian,
  prior_intercept = normal(51, 2.5, autoscale = TRUE),
  prior = normal(0, 2.5, autoscale = TRUE), 
  prior_aux = exponential(1, autoscale = TRUE),
  prior_PD = FALSE,
  chains = 4, iter = 5000*2, seed = 83)

# 100 posterior model lines with wind speed
Pablo |> drop_na() |> 
  add_fitted_draws(pen_model, n = 100) |> 
  ggplot(aes(x = bill_length_mm, y = flipper_length_mm)) +
    geom_line(aes(y = .value, group = .draw), alpha = 0.15)
```
b) Density plots for all penguins and Pablo. 

```{r}
# Trace plots of parallel chains for penguin_bayes
mcmc_trace(pen_model, size = 0.1)

# Density plots of parallel chains
mcmc_dens_overlay(pen_model)

# Trace plots of parallel chains
mcmc_trace(pablo_model, size = 0.1)

# Density plots of parallel chains
mcmc_dens_overlay(pablo_model)
```

The chains look mostly stable and the density plots are narrow and show a normal distribution which increases confidence in these posteriors. 

c) 
```{r}
tidy(pen_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)

tidy(pablo_model, effects = c("fixed", "aux"),
     conf.int = TRUE, conf.level = 0.80)
```


The 80% CI for flipper length among all species is between 120.74 and 132.67. The 80% CI for flipper length is between 142.37 and 159.76 for the species designated as Pablo. Pablo has longer flipper lengths.For all penguins with 51 mm, their credible interval will be narrower. since that data set would match the prior average. 

e) 
```{r}

#posterior predictions of penguins and pablo
set.seed(51)
shortpen_prediction <- 
  posterior_predict(pen_model, newdata = data.frame(bill_length_mm = 51))
posterior_interval(shortpen_prediction, prob = 0.80)

#pablo
set.seed(51)
shortpab_prediction <- 
  posterior_predict(pablo_model, newdata = data.frame(bill_length_mm = 51))
posterior_interval(shortpab_prediction, prob = 0.80)

```


## Exercise 9.20

a) Their prior understanding is that as body mass increases so does flipper length by a  small amount and with a small standard deviation. 

b) 
```{r}
data("penguins_bayes")
ggplot(penguins_bayes, aes(x = body_mass_g, y = flipper_length_mm)) +
  geom_point(size = 0.1) + 
  geom_line(method = "lm", se = FALSE)
```

c)

