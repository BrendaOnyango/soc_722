---
title: "Ch. 14 HW due 2.27.22"
author: "Brenda Onyango"
date: "2/25/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Chapter 14 Homework: Matching

### 1: Matching with Penmanship Data

a. Create a set of weights for handedness groups so that treatment matches control.

I will give a weight of 1 to everyone in the control group.  


b. Weights for left, right, and ambidextrous in treated group.

```{r}
leftweighttreat <- 10/6 #control overtreated for each group to find weight
rightweighttreat <- 88/90
ambweighttreat <- 2/4

leftweighttreat 
rightweighttreat 
ambweighttreat

```

c. Use the weights in part b to calculate proportion of left-handed and other people in the treated group.

```{r}
lefttreatprop <- 6*leftweighttreat
rightreatprop <- 90*rightweighttreat
ambidextprop <- 4*ambweighttreat #this was number of observations times weight to find proportion

lefttreatprop 
rightreatprop 
ambidextprop 
```

d. weighted average penmanship in treated group 
```{r}
# doing sum of weights times observation divided by sum of weights 

wapt <- (leftweighttreat*7 + rightweighttreat*6 + ambweighttreat*4)/(leftweighttreat + rightweighttreat + ambweighttreat)

wapt
```

e. 
If we use our weighted data then our ATUT (because we're matching treated observations to control observations) of practicing cursive is the below on  a 10 point scale:

```{r}
practiceffect <- wapt - 5 #5 is the average penmanship of people who don't practice 
practiceffect
```

I also ran the below code to see what the unweighted effect would be:

```{r}
unweightedteffect <- (.06*7) + (.9 * 6) + (.04*4)
unweightedteffect
```

The unweighted ATUT is:

```{r}
unweightedate <-unweightedteffect - 5 #5 is average penmanship of people who don't practice 
unweightedate
```



### 2: Types of Matching 

a. kernel matching 

b. k-nearest-neighbor matching with k = 2 

c. propensity score matching 

d. one-to-one distance matching

### 3: Bias v. Variance

a. (B) selecting multiple control matches for each treatment *increases* bias. (A) selecting one control match for each treatment increases variance and decreases bias.


b. (A) using a relatively wide bandwidth *increases* bias. (B) Using a narrower bandwidth increases variance and decreases bias. 


c. (B) selecting matches without replacement *increases* bias. (A) selecting matches with replacement *increases* variance and *reduces* bias. 


d. (B) applying a weight that accepts many controls but decays with distances *increases* bias and reduces variance. (A) selection one control match for each treatment increases variance and decreases bias. 

### 4: Exact Matching 

Exact matching (coarsened exact matching) should be reserved for very large samples or situations where a very small number of matching variables is appropriate because if applied to samples with moderate to few observations, treatment observations will be dropped. This leads to the average treatment effect being inaccurate since there is bias in which treated observations find matches. If using this method, it's imporntnat to see how many treated observations are lost. 

### 5: Mahalanobis distance

The causal path we're interested in is sports --> grades. We are matching on 5 variables using Mahalanobis distance. 

A caliper/bandwidth of 0.3 means that the treated observation and control observations should be within 0.3 standard deviations of each other when matched. 

1) To find Mahalanobis distance we found the difference between observed and control observations for high school athleticism, parental income, gender, race, and middle school grades. 

2) Then  we use the distance formula for two points in matrix notation for all the differences of the 5 matched variables.The answer we get is the distance. 

3) We then compare this distance between treated observations and untreated observations and match with treated and untreated observations based on the lowest distance. 

3.5) Standard error adjustments. 

4) After creating matched datasets we can calculate the average treatment effect of sports participation by finding the following difference:effect on sportsplayers - effect on control = .1 grade points.

### 6: Downside of Propensity Score Matching 

d. It requires that the model used to estimate the propensity score is properly specified. 

I eliminated a because it's possible to combine both methods. I eliminated b because the point of matching, regardless of method, is to close back doors. I eliminated c because if we use inverse probability weighting with propensity score (there are other ways to find propensity scores) then we do use weights. D is true in that distance matching, the other main approach of matching we reviewed, doesn't require a regression for propensity score. An incorrect regression (not the right variables or powers) for the propensity score will lead to an unhelpful propensity score. 

### 7: Tax-Rebate for Small Businesses 

a. The common support assumption is that there are are appropriate control observations to match to treated observations. The group of businesses with 1-5 employees fails the common support assumption. 

b. Not having treated retail businesses with 11-20 employees is not a concern for the common support assumption if we're trying to estimate *average treatment on the treated* since we would only be interested in the six retailers and 11 services that were treated. 

c. With only one untreated service business in the 11-20 employee category we would be concerned about having to default to matching with replacement. Our sampling variation will be large i.e. "the sample mean for the controls would just be that one observation's outcome value, and...standard error...equal to the standard deviation" even if bias is low. 

d. If we trim to resolve the common support problem then we create the problem that we are no longer estimating an average treatment effect or an average treatment effect on the treated, but rather a treatment effect "among the individuals that match well." If we have to drop a lot of data to make matching work we may have back doors we are not able to close and maybe should use another stats method. 

### 8: Matching on Schooling Reform 

a. There are no statistically significant differences in expenditure and income at the 95% level. 

b. I would consider the following two things:
  1) visual diagnostics - if we had the observations for income in the treated and control group a visual plot like a density plot could help me see if there are any regions that are imbalanced. 
  
  2) I would want to see the table with raw, un-matched data to see the extent of the balance problem matching was supposed to solve. If the unmatched means for income in the treatment and control group have a similar difference to the matched means then maybe we have not fixed the balance problem. 
  
c. My next step would be to revisit the regression for the propensity score to see if I closed the backdoors that I intended to. 

### 9: ATT v. ATUT 

Selecting untreated observations to match treated observations produces an average treatment effect on the treated (ATT) because we are comparing what we got from treatment against a group that was untreated. The reverse, matching treated observations to control observations, gives a treated group that is as similar to the untreated group as possible. Comparing the treatment and control in this scenario means we're comparing the effect from no treatment to what we think the effect the control group would have if they were treated - an average treatment on the untreated.  

## Ch. 14 Coding Homework 

```{r}

#loading packages we used in class 
library(MatchIt)
library(WeightIt)
library(cobalt)
library(tidyverse)
library(broom)
library(haven)
```
```{r}
#viewing dataset 

library(causaldata)
data("nsw_mixtape")
view(nsw_mixtape)
```


### 1: Drop variable from the data

```{r}
#removing variable"data_id" from data set 
mixtape_noID <-  nsw_mixtape |> select(-data_id)
```


### 2

First, create a variable called `weight` in your data equal to 1 for all observations (weights that are all 1 will have no effect, but this will give us a clue as to how we could incorporate matching weights easily).

```{r}
mixtape_noID <- mixtape_noID |> mutate(mixtape_noID, weight = 1)
view(mixtape_noID)
```

Second, write code that uses a set of given weights to estimate the effect of `treat` on `re78`, using `weight` as weights, and prints out a summary of the regression results.

```{r}
library(modelsummary)
```


```{r}

m1 <- lm(re78 ~ treat + age + educ + black + hisp + marr + nodegree + re74 + re75 + I(re74^2) + I(re75^2) + I(age^2) + I(educ^2), #effect of treatment (x) on re78(y) with some variables squared like Steve did in class to improve the model
         data = mixtape_noID,
         weights = weight) #using weight as weights

msummary(m1,
         stars = TRUE) #summary of regression results



```


Third, write code that creates and prints out a weighted balance table for all variables across values of `treat`, using `weight` as weighted. Get the means.

```{r}
weight1 <- weightit(treat ~ age + educ + black + hisp + marr + nodegree + re74 + re75 + I(re74^2) + I(re75^2) + I(age^2) + I(educ^2),
                    data = mixtape_noID,
                    weights = mixtape_noID$weight) #using weight as weighted
```

```{r}
bal.tab(weight1, disp = c("means", "sds"), #including mean and stdev in balance table
       stats = c("mean.diffs", "variance.ratios") )

```

Let's also do a plot.

```{r}
bal.plot(weight1,
         which = "both",
         mirror = TRUE)

love.plot(weight1,
          abs = TRUE,
          binary = "std",
          thresholds = .1)
```


### 2b 

Is there anything potentially concerning about the balance table, given that this is a randomized experiment where `treat` was randomly assigned?

Looking at the loveplot it's concerning that the unadjusted 'nodegree' and 'hispanic', and 'age' variables are have a lot of imbalance. These and other variables have arrows going to treatment and to the outcome re78 on a DAG. 

### 3

Using all of the variables in the data except `treat` and `re78` as matching variables, perform 3-nearest-neighbor Mahalanobis distance matching with replacement and no caliper and calculate the post-matching average treatment on the treated effect of `treat` on `re78`.

```{r}
match2 <- matchit(treat ~ age + educ + black + hisp + marr + nodegree + re74 + re75,
                  data = mixtape_noID,
                  method = "nearest",
                  distance = "mahalanobis",
                  estimand = "ATT",
                  replace = TRUE) 
```

```{r}
#summary of Mahalanobis distance matching 
summary(match2,
        un = FALSE,
        improvement = FALSE)
```


```{r}
#doing loveplot for balance check since I'm following process we were shown in class
love.plot(match2,
          abs = TRUE,
          binary = "std",
          thresholds = .1) #looks good in that the adjusted values are below the threshold 
```


```{r}
#getting matched data for calculating weighted difference 

matcheddata <- match.data(match2)
```

```{r}
#now I can find the post-matching average treatment on the treated effect of treat on re78

pstmatchatt <- lm(re78 ~ treat,
                 data = matcheddata,
                 weights = weights)
tidy(pstmatchatt, conf.int = TRUE)
```


### 4

Create a post-matching balance table showing balance for all the matching variables (you'll probably want to use the balance function designed to follow the matching function you used, from the same package). Write a sentence commenting on whether the balance looks good. You may have to read the documentation for the function you use to figure out what the results mean.

```{r}
bal.tab(match2, disp = c("means", "sds"), #including mean and stdev in balance table
       stats = c("mean.diffs", "variance.ratios") )


```


This table and my earlier loveplot as part of the answer for 3 show me that the balance looks good. The loveplot shows that all the variables were under the .1 threshold. 

#### Not part of Exercise - just seeing the means 

  Trim the propensity score, setting to missing any values from 0 to .05 or from .95 to 1 (this is a different method than done in the chapter). Make sure the values are all between 0 and 1.

First I'll do the pre-analysis in a different way to see means between treated and untreated people in the set on our 

```{r}
mixtape_noID |> group_by(treat) |> summarise(mean_re78 = mean(re78))
```

This was good for seeing how the unmatched means differ between the treated and untreated in an uncluttered way. 

### 5

Switching over to propensity score matching, use the same matching variables as in Question 3 to estimate the propensity to be treated (with a logit regression)... 

```{r}
treatmentpropen <- glm(treat ~ age + educ + black + hisp + marr + nodegree + re74 + re75,
            family =binomial(), data = mixtape_noID)
summary(treatmentpropen) #step 1 was creating a model with covariates for the propensity score 
```

and then add the treatment propensity to the data set as a new variable called `propensity`...

```{r}
propen_df <- data.frame(propensity = predict(treatmentpropen, type = "response"),
                        treat = treatmentpropen$model$treat)

head(propen_df) #viewing the first 5 and last 5 of the dataframe with propensity scores

tail(propen_df)
```

```{r}
#adding propensity column to mixtape_noID

mixtape_noID <- mixtape_noID |> inner_join(propen_df, by = "treat")


view(mixtape_noID)

```


Trim the propensity score, setting to missing any values from 0 to .05 or from .95 to 1 (this is a different method than done in the chapter).

```{r}
library(naniar)
```


```{r}

replace_with_na_all(mixtape_noID$propensity, ~.x <0.05) 
replace_with_na_all(mixtape_noID$propensity, ~.x >0.95)

#were there values that met these conditions they would have been trimmed. 
```


### 6 

Create a new variable in the data called `ipw` with the inverse probability weight...

```{r}
mixtape_noID <- mixtape_noID |> mutate(ipw = case_when (
  treat == 1 ~ 1/propensity,
  treat == 0 ~ 1/(1-propensity)
))

tail(mixtape_noID)
head(mixtape_noID)
```



and then estimate the treatment effect using those weights in a linear regression (keeping in mind the standard errors won't be quite right).

```{r}

finalset <- mixtape_noID 

m2 <- lm(re78 ~ treat,
         data = finalset,
         weights = finalset$ipw)
tidy(m2, conf.int = TRUE)

msummary(m2,
         stars = TRUE) 
```


### 7

 Make a common support graph, overlaying the density of the `propensity` variable for treated observations on top of the density of the `propensity` variable for untreated observations.
 
```{r}
weight2 <- weightit(treat ~ age + educ + black + hisp + marr + nodegree + re74 + re75,
                    data = finalset,
                    method = "ps",
                    estimand = "ATT")

summary(weight2,
        un = FALSE,
        improvement = FALSE)
```
 
```{r}
bal.plot(weight2,
         which = "both", #this bal.plot shows areas of common support
         mirror = TRUE)


love.plot(weight2,
          abs = TRUE,
          binary = "std",
          thresholds = .1)
```


The above plot shows we've met our assumption of common support and the love plot shows we definetly met our threshold of 0.1 even for variables with large standardized mean differences when unadjusted like nodegree. 

### 8 

Use the prepackaged command for inverse probability weighting used in the chapter for your language to estimate the treatment effect. 

```{r}
#computing weighted difference ATT
m3 <- lm(re78 ~ treat, data = finalset, weights = ipw)

msummary(m3, 
         stars = TRUE)
```


