---
title: "HW_5: Bayes' Rule"
author: "Brenda Onyango"
date: "9/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

## Chapter 1 Exercises 

### 1.1

a) Prior information for Leslie's story includes that cows produce milk and cows come in different colors. 

b) Leslie weighed data on the color of milk and cows, and on where chocolate flavors can come from against the incoming information about chocolate syrup.

c) The updated conclusion is that all cows, regardless of hide color produce white milk, and chocolate syrup is added to create chocolate flavored milk.

### 1.2
a) I thought preparing Thanksgiving dinner was easy until I tried it one year. #Bayesiantweet

b) Nine times out of ten to going to the DMV is a terrible experience. #Bayesiantweet

### 1.3

```{r}
library("tidyverse") #loading packages needed to create diagram
library("DiagrammeR")
library("dplyr")
```

I changed my mind on the relative safety of Tesla to other car brands. Below is a diagram about this change. 

#### Diagram code 
```{r}
tree <-
    create_graph() |>  # initiate graph
    add_n_nodes(
      n = 3, 
      type = "path",
      label = c("Incoming info: Marketing from Tesla that car is safe", "Data: Tesla combustion events", "Posterior: Tesla cars not as safe as other brands"), # Labels for each node
      node_aes = node_aes(
        shape = "rectangle", #changed shape to rectangle 
        height = 2,
        width = 2,
        x = c(0, 3, 3), 
        y = c(0, 2, -2))) |> 
    add_edge(
      from = 1,
      to = 3,
      edge_aes = edge_aes(
        color = "blue"
      )) |>  
  add_edge(
      from = 2,
      to = 3,
      edge_aes = edge_aes(color = "green")
      )
render_graph(tree)
```

### 1.4

I convinced a friend to try a restaurant specializing in salads. 

```{r}
tree <-
    create_graph() |>  # initiate graph
    add_n_nodes(
      n = 3, 
      type = "path",
      label = c("Friend prior info:Salads are not filling", "Data: Me sharing about restaurant experience", "Posterior: Friend agrees salads can be filling"), # Labels for each node
      node_aes = node_aes(
        shape = "rectangle", #changed shape to rectangle 
        height = 2,
        width = 2,
        x = c(0, 3, 3), 
        y = c(0, 2, -2))) |> 
    add_edge(
      from = 1,
      to = 3,
      edge_aes = edge_aes(
        color = "blue"
      )) |>  
  add_edge(
      from = 2,
      to = 3,
      edge_aes = edge_aes(color = "green")
      )
render_graph(tree)
```

### Exercise 1.5

Draw diagram about author's changing views on Bayes. 

```{r}
tree <-
    create_graph() |>  # initiate graph
    add_n_nodes(
      n = 7, 
      type = "path",
      label = c("incoming: no thoughts on Bayes", "data:Bayes for diagnostics", "Bayes is interest", "data: ant egg exercise", "Bayes disinterest", "data: great PhD prof", "updated: Bayes is interest"), # Labels for each node
      node_aes = node_aes(
        shape = "oval",
        height = 1,
        width = 1,
        x = c(0, 2, 3, 5, 4, 6, 5), # adjusted node positions
        y = c(6, 6, 5, 5, 4, 4, 3))) %>% 
    add_edge(
      from = 1,
      to = 3,
      edge_aes = edge_aes(
        color = "black"
      )) |> 
  add_edge(
      from = 2,
      to = 3,
      edge_aes = edge_aes(
        color = "black"
      )) |> 
  add_edge(
      from = 3,
      to = 5,
      edge_aes = edge_aes(
        color = "black"
      )) |>  
  add_edge(
      from = 4,
      to = 5,
      edge_aes = edge_aes(
        color = "black"
      )) |> 
  add_edge(
      from = 5,
      to = 7,
      edge_aes = edge_aes(
        color = "black"
      )) |>  
  add_edge(
      from = 6,
      to = 7,
      edge_aes = edge_aes(
        color = "black"
      )) 
render_graph(tree)
```
### Exercise 1.6 

a) A frequentist would ask/answer what is the probability that I'm qualified if I get the job. 

b) A Bayesian would ask/answer what is the probability that I'll get the job given that I'm qualified. 

c) I fall somewhere in the middle on preference. The Bayesian thinking is useful for thinking about the near term of this situation. Frequentist thinking could help decide if I should increase my qualifications for long term job prospects. 

### Exercise 7

a) I already know about health policy.

b) Some people hypothesize that healthcare utilization is based on medical needs. 

c) Because of knowledge in this area I know that when insured people have lower cost sharing, they increase healthcare utilization. I also know that people without insurance forego needed healthcare because the cost is prohibitive. Therefore, healthcare utilization in the US is not based just on immmediate medical needs but is heavily moderated by insurance status and cost sharing. 

d) I employed Bayesian thinking (adding data to an priori assumption that people seek care when they need it). 

### 1.8

a) Bayesian statistics is useful for updating what we know about probabilities for events of interest when new or additional information is added. It is an application of conditional probability. 

b) Bayesian and frequentist thinkers both want to use data to generate knowledge about the world. Both test hypotheses, create models, and make predictions. 

## Chapter 2 Exercises 

### Exercise 2.1

a) P(B|A) > P(B) In this example the probability that I will enjoy Benn's newest novel given that I've read the first novel and enjoyed it is greater than the probability that I will enjoy the new novel without any information about the author. 

b) P(B|A) < P(B) The probability that it will be 60 degrees tomorrow given that it's 0 degrees Fahrenheit in Minnesota in January is less than the probability that it will be 60 degrees tomorrow. I know that seasonal weather patterns don't usually mean 60 degree differences in the weather from day to day which makes the conditional probability less likely. 

c) P(B|A) > P(B) The probability that the authors make several typos in their writing today given they only got 3 hours of sleep is greater than the probability the authors make several typos. The information I'm adding is I know sleepiness affects accuracy. 

d) P(B|A) > P(B) The probability that the tweets get retweeted given that my friend uses three hashtags is greater than the probability that the tweet gets retweeted. I know adding hashtags increases the chance of getting retweeted. 

### Exercise 2.2

a) P(B|A) probability of getting a speeding ticket given driving 10 mph above the speed limit. It's conditional.

b) P(A) = .20. This is the probability of residents that drive 10 mph above the speed limit. It's marginal. 

c) P(D) = .15. This is the probability that a resident has used R. This is marginal.

d) P(D|C) Probability of using R given that you've taken statistics at local college. This is conditional. 

e) P$(F \cap E)$ This is joint probability of being a Minnesotan and liking Prince. 

d) P(E|F). This conditional probability is that of liking Prince's music given that you're a Minnesotan. 

### Exercise 2.3 

To use a binomial model several conditions need to be met: the outcome of on trial does not influence another, and the probability of success on each trial is pi, know the number of trials 

a) NOT binomial. We don't know the probability for an individual event of a child being born and given variance in pregnant people's health and that of their fetuses the chance of being born is also variable among those in labor. 

b) Binomial. 	$Y|\pi \sim {\sf Binom}(27, .90)$

c) NOT binomial. We have \pi of .17 but we don't have the number of trials. 

d) NOT binomial. We don't have the number of trials that Henry is late or on time and we don't have a \pi. 

e) NOT binomial. We don't have a value for Y and we don't have a number of trials of a party being thrown. 

f) Binomial. $Y|\pi \sim {\sf Binom}(60, .8)$

### Exercise 2.5.2

2.4) Let A = sparkles like diamond and B = vampires exist.
We are given P(B) = 0.05, P(A|B) = 0.70 and P(A|B^c) = .03.

```{r}
pB <- 0.05
pAgivenB <- 0.70
pAgivenNOTB <- 0.03
pNOTB <- 0.95 #we know this because of complement or 1-.95
```

I found P(A) or P(sparkles like diamond) using the properties of  total probablility 

```{r}
(0.70*0.05) + (0.03*0.95) #P(A|B)P(B) +P(A|B^c)(P(B^c)
```

```{r}
pA <- 0.0635 #setting P(A) to value found in last chunk
```

Now solving using 
P(B|A) = P(A|B) P(B) / P(A) 
```{r}
pAgivenB*pB/pA
```

2.5) 
a) The prior probability of a tree having mold is 0.18.

b) We are given the following where M means mold infection and asked to find p(maple):

```{r}
pM <- 0.18
pNOTM <- 0.82 #found using 1-.18
pelmgivenM <- .15
pmaplegivenM <- 0.80
pothergivenM <- .05
pelmNOTM <- 0.20
pmapleNOTM <- .10
potherNOTM <- .70 

```

b) To find p(maple) I used law of total probability.

```{r}
(pmaplegivenM*pM) + (pmapleNOTM*pNOTM)
```
c) To find posterior that a maple tree has mold I will use P(B|A) = P(A|B) P(B) / P(A) 

```{r}
pmaplegivenM*pM/0.226
```

d) p(mold) < p(mold|maple) meaning that knowing the tree is a maple increased the probability there is mold. Most of the trees in the sample are maple and across all trees p(mold) = .18 so we might want to explore why p(mold|maple) is high besides being overrepresented in the arboretum. 

### Exercise 2.6 

We need to know among the restaurants she does not like what's the probability of four stars or less p(4stars|not like). 

### Exercise 2.7

We're given:

```{r}
pswipe <- 0.08
pNOTswipe <- 0.92
pmengivenswipe <- 0.40
pwomengivenswipe <- 0.30
pnbgivenswipe <- 0.20
pothergivenswipe <- .10
pmenNOTswipe <- .45
pwomenNOTswipe <- .40
pnbNOTswipe <- 0.10
potherNOTswipe <- 0.05
```

a) find p(nonbinary). I will use total probability to find this. 

```{r}
(pnbgivenswipe*pswipe) + (pnbNOTswipe*pNOTswipe)
```

B) find p(swipe|nonbinary). For this I will use P(B|A) = P(A|B) P(B) / P(A) where A is nonbinary and B is swipe.

```{r}
pnbgivenswipe*pswipe/0.108
```

### Exercise 2.8

We're given
```{r}
pmorn <- 0.30
pdelay <- 0.15
pNOTdelay <- 0.85
pmorngivendelay <- 0.40
```

a) find P(delay|morning). I will use P(B|A) = P(A|B) P(B) / P(A) where A is morning and B is delay. 

```{r}
pmorngivendelay*pdelay/pmorn
```
The probability Mine's flight is delayed =0.20.

b) find p(morning|NOTdelay). For this I'll use total probability and Bayes. First I find p$(morning \cap NOTdelay)$ using pmorn - (pmorngivendelay*pdelay)
```{r}
pmorn - (pmorngivendelay*pdelay)
```
Then I can do p(norning|NOTdelay) = p$(morning \cap NOTdelay)$ / pNOTdelay 

```{r}
0.24/ pNOTdelay 
```
The probability Alicia is on a morning flight =.282.

### Exercise 2.9 

a) I will fill in the table using code written with help from Braulio. 

```{r}
library("tibble")
goodmood <- c(0.05, 0.84, 0.11)
badmood <- c(0.13, 0.86, 0.01)

table_r <- tibble(good = goodmood*0.40,
                  bad = badmood*0.60) #0.40 is prior probability of good mood and 0.60 is prior probability of bad mood found from 1-0.40 because of total probability. Multiplying conditional probability by prior gives us the joint probabilities for the table.  
table_r
```

b) Without any information about texts the prior probability is 0.40 that the roommate is in a good mood. 

c) We're asked for p(hightexts|goodmood) where hightexts is receiving 46+ texts. This is liklihood and it was given as 0.11. 

d) Find (goodmood|hightexts) which is posterior probability. We need to find p(hightexts) using law of total probability 
```{r}
(0.11*0.40) + (0.01*0.60)
```


```{r}
0.40 * 0.11/.05 #0.05 is p(hightexts)
```

### Exercise 2.10

Find pLGBTQ given:
```{r}
prural <- 0.085
purban <- 0.915
pqueergivenrural <- 0.100
pqueergivenurban <- 0.105
```

a) use total probability.
```{r}
pqueergivenrural*prural + pqueergivenurban*purban
```

b) find P(rural|LGBT) 
```{r}
pqueergivenrural*prural/0.104575 #Bayestheorem
```

c) p(rural|notLGBT)
```{r}
1-0.104575 #complement
```
Setting notLGBT 
```{r}
notqueer <- 0.895425
```
Use Bayes theorem.

```{r}
prural*0.90/notqueer #0.90 is p notqueergivenrural and is found from 1-pqueergivenrural 
```

## Exercise 2.5.3

a) $Y|\pi \sim {\sf Binom}(6, \pi)$ and conditional pmf is

 f(y|$\pi$) = C(6,y) $\pi$^y * (1- $\pi$)^(6-y) for $ y\in $ {0, 1, 2, 3, 4, 5, 6} 
 
b)  using dbinom function and given successes = 4 and \pi = .3 we can find how likely this is. 
 
```{r}
dbinom(4,6,.3) #where 4 is number of successes, 6 is trials and pi is .3
```
 
### Exercise 2.12
 
 a) $Y|\pi \sim {\sf Binom}(7, \pi)$ and conditional pmf is f(y|$\pi$) = C(7,y) $\pi$^y * (1- $\pi$)^(7-y) for $ y\in $ {0, 1, 2, 3, 4, 5, 6,7} 
 

 
 
 